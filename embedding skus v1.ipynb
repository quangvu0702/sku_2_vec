{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKU TO VEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "https://arxiv.org/pdf/1103.0398.pdf\n",
    "\n",
    "http://sebastianruder.com/word-embeddings-1/index.html#continuousbagofwordscbow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding models\n",
    "\n",
    "Naturally, every feed-forward neural network that takes words from a vocabulary as input and embeds them as vectors into a lower dimensional space, which it then fine-tunes through back-propagation, necessarily yields word embeddings as the weights of the first layer, which is usually referred to as Embedding Layer.\n",
    "\n",
    "The main difference between such a network that produces word embeddings as a by-product and a method such as word2vec whose explicit goal is the generation of word embeddings is its computational complexity. Generating word embeddings with a very deep architecture is simply too computationally expensive for a large vocabulary. This is the main reason why it took until 2013 for word embeddings to explode onto the NLP stage; computational complexity is a key trade-off for word embedding models\n",
    "\n",
    "#### 1 Embedding Layer: \n",
    "A layer that generates word embeddings by multiplying an index vector with a word embedding matrix;\n",
    "#### 2 Intermediate Layer(s):\n",
    "One or more layers that produce an intermediate representation of the input, e.g. a fully-connected layer that applies a non-linearity to the concatenation of word embeddings of nn previous words;\n",
    "#### 3 Cost Layer:\n",
    "The final layer that produces a probability distribution over words in VV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW model\n",
    "\n",
    "<img src=\"img/cbow.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pairwise ranking criterion, which looks like this:\n",
    "\n",
    "   <img src=\"img/margin_1_loss.png\">\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, config variables, and data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global config variables\n",
    "batch_size = 10 # 128\n",
    "num_classes = 10 # number of skus and zuids ~700.000 \n",
    "state_size = 4 # 32, 64, 128\n",
    "learning_rate = 0.1 \n",
    "d_win = 1\n",
    "n_negative = 7\n",
    "n_positive = 1\n",
    "\n",
    "#layer_1 \n",
    "n_layer_1 = 8\n",
    "\n",
    "#layer_2\n",
    "n_layer_2 = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data(size=128):\n",
    "    pass\n",
    "\n",
    "def gen_batch(raw_data, batch_size, num_steps):\n",
    "    pass\n",
    "\n",
    "def gen_epochs(n, num_steps):\n",
    "    for i in range(n):\n",
    "        yield gen_batch(gen_data(), batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we treat a continuous sequence of actions of a user in a transaction as a sentences, the sku will play a role as a word.\n",
    "<img src=\"img/model_v1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "d_win = tf.placeholder(tf.int32, [batch_size, d_win * 2], name = 'context')\n",
    "negative_sample = tf.placeholder(tf.int32, [batch_size, n_negative], name = 'negative_sample')\n",
    "positive_sample = tf.placeholder(tf.int32, [batch_size, n_positive], name = 'positive_sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-functions for creating new variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]))\n",
    "\n",
    "def new_fc_layer(input,          # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs,    # Num. outputs.\n",
    "                 use_relu=True): # Use Rectified Linear Unit (ReLU)?\n",
    "\n",
    "    # Create new weights and biases.\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    # Use ReLU?\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "\n",
    "#lookup table \n",
    "embeddings = new_weights([num_classes, state_size])\n",
    "\n",
    "# input vectors\n",
    "d_win_vec = tf.nn.embedding_lookup(embeddings, d_win)\n",
    "negative_sample_vec = tf.nn.embedding_lookup(embeddings, negative_sample)\n",
    "positive_sample_vec = tf.nn.embedding_lookup(embeddings, negative_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
