{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKU TO VEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "https://arxiv.org/pdf/1103.0398.pdf\n",
    "\n",
    "http://sebastianruder.com/word-embeddings-1/index.html#continuousbagofwordscbow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding models\n",
    "\n",
    "Naturally, every feed-forward neural network that takes words from a vocabulary as input and embeds them as vectors into a lower dimensional space, which it then fine-tunes through back-propagation, necessarily yields word embeddings as the weights of the first layer, which is usually referred to as Embedding Layer.\n",
    "\n",
    "The main difference between such a network that produces word embeddings as a by-product and a method such as word2vec whose explicit goal is the generation of word embeddings is its computational complexity. Generating word embeddings with a very deep architecture is simply too computationally expensive for a large vocabulary. This is the main reason why it took until 2013 for word embeddings to explode onto the NLP stage; computational complexity is a key trade-off for word embedding models\n",
    "\n",
    "#### 1 Embedding Layer: \n",
    "A layer that generates word embeddings by multiplying an index vector with a word embedding matrix;\n",
    "#### 2 Intermediate Layer(s):\n",
    "One or more layers that produce an intermediate representation of the input, e.g. a fully-connected layer that applies a non-linearity to the concatenation of word embeddings of nn previous words;\n",
    "#### 3 Cost Layer:\n",
    "The final layer that produces a probability distribution over words in VV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW model\n",
    "\n",
    "<img src=\"img/cbow.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pairwise ranking criterion, which looks like this:\n",
    "\n",
    "   <img src=\"img/margin_1_loss.png\">\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, config variables, and data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global config variables\n",
    "batch_size = 16 # 128\n",
    "num_classes = 10 # number of skus and zuids ~700.000 \n",
    "state_size = 4 # 32, 64, 128\n",
    "learning_rate = 0.1 \n",
    "d_win = 1\n",
    "n_negative = 7\n",
    "n_positive = 1\n",
    "\n",
    "#layer_1 \n",
    "n_layer_1 = 8\n",
    "\n",
    "#layer_2\n",
    "n_layer_2 = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data(size=128):\n",
    "    pass\n",
    "\n",
    "def gen_batch(raw_data, batch_size, num_steps):\n",
    "    pass\n",
    "\n",
    "def gen_epochs(n, num_steps):\n",
    "    for i in range(n):\n",
    "        yield gen_batch(gen_data(), batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we treat a continuous sequence of actions of a user in a transaction as a sentences, the sku will play a role as a word.\n",
    "<img src=\"img/model_v1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-functions for creating new variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]))\n",
    "\n",
    "def new_fc_layer(input,          # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs,    # Num. outputs.\n",
    "                 use_relu=True): # Use Rectified Linear Unit (ReLU)?\n",
    "\n",
    "    # Create new weights and biases.\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    # Use ReLU?\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayersian Personalize Ranking (BPR)\n",
    "\n",
    "Loss function:\n",
    "\n",
    "<img src=\"img/BPR_loss.png\">\n",
    "\n",
    "with:\n",
    "\n",
    "<img src=\"img/BPR_loss_2.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "u_index = tf.placeholder(tf.int32, [batch_size, 1], name = 'zuid')\n",
    "x_i_index = tf.placeholder(tf.int32, [batch_size, 1], name = 'xi')\n",
    "x_j_index = tf.placeholder(tf.int32, [batch_size, 1], name = 'xj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "\n",
    "#lookup table \n",
    "embedding_skus = new_weights([num_classes, state_size])\n",
    "embedding_zuid = new_weights([num_classes, state_size])\n",
    "# input vectors\n",
    "u = tf.nn.embedding_lookup(embedding_zuid, u_index)\n",
    "x_i = tf.nn.embedding_lookup(embedding_skus, x_i_index)\n",
    "x_j = tf.nn.embedding_lookup(embedding_skus, x_j_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/BPR_loss_2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_ui = tf.matmul(u, x_i, transpose_a=False, transpose_b=True)\n",
    "x_ui = tf.reshape(x_ui, [-1])\n",
    "x_uj = tf.matmul(u, x_j, transpose_a=False, transpose_b=True)\n",
    "x_uj = tf.reshape(x_uj, [-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/BPR_loss.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_uij = tf.minimum(x_ui - x_uj, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost-function to be optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = - tf.reduce_mean(x_uij)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support function AUC estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/AUC.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AUC for each user:\n",
    "\n",
    "# average AUC:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
